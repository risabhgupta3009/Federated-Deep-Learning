{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>FEDERATED LEARNING MODEL</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>IMPORTING LIBARIES</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from imutils import paths\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import * \n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preparing DataSet</h3>\n",
    "\n",
    "<p>Step 1 : Extracting Images from the  Folders</p>\n",
    "<p>Step 2 : Adjusting The angles</p>\n",
    "<p>Step 3 : Normalizing the Image</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 224 images belonging to 2 classes.\n",
      "Found 60 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN_PATH = \"CovidDataset/Train\"\n",
    "VAL_PATH = \"CovidDataset/Test\"\n",
    "\n",
    "train_datagen = image.ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    ")\n",
    "test_dataset = image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'CovidDataset/Train',\n",
    "    target_size = (224,224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary')\n",
    "\n",
    "\n",
    "validation_generator = test_dataset.flow_from_directory(\n",
    "    'CovidDataset/Val',\n",
    "    target_size = (224,224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary')\n",
    "\n",
    "\n",
    "\n",
    "#split data into training and test set\n",
    "X_train,Y_train = next(train_generator)\n",
    "X_test,Y_test = next(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Creating Clients</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    client_names = ['{}_{}'.format( initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}\n",
    "\n",
    "clients = create_clients(X_train, Y_train, num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating CNN Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build():\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(224,224,3)))\n",
    "        model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64,activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hashiraflame/.local/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01 \n",
    "comms_round = 100\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(lr=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calculating the Weights for each Clients</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test,model):\n",
    "    pred_img = model.predict(X_test)\n",
    "    print(np.argmax(pred_img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calculating the weights of the Global Model for each Clients</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 240s 38s/step - loss: 2.2184 - accuracy: 0.5402 - val_loss: 0.6867 - val_accuracy: 0.9167\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 54s 7s/step - loss: 0.6601 - accuracy: 0.6339 - val_loss: 0.6479 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 61s 9s/step - loss: 0.5779 - accuracy: 0.6875 - val_loss: 0.4662 - val_accuracy: 0.8667\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 75s 11s/step - loss: 0.3415 - accuracy: 0.8616 - val_loss: 0.2311 - val_accuracy: 0.9333\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 45s 6s/step - loss: 0.4006 - accuracy: 0.8393 - val_loss: 0.3930 - val_accuracy: 0.9667\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 54s 8s/step - loss: 0.3274 - accuracy: 0.8571 - val_loss: 0.1812 - val_accuracy: 0.9667\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 44s 6s/step - loss: 0.3932 - accuracy: 0.8170 - val_loss: 0.3436 - val_accuracy: 0.9667\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 47s 6s/step - loss: 0.2607 - accuracy: 0.8973 - val_loss: 0.1164 - val_accuracy: 0.9667\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 68s 8s/step - loss: 0.7380 - accuracy: 0.5982 - val_loss: 0.6825 - val_accuracy: 0.8667\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 44s 6s/step - loss: 0.5913 - accuracy: 0.7321 - val_loss: 0.5239 - val_accuracy: 0.9333\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 43s 6s/step - loss: 0.4589 - accuracy: 0.7723 - val_loss: 0.3580 - val_accuracy: 0.9500\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 42s 6s/step - loss: 0.2628 - accuracy: 0.9196 - val_loss: 0.3479 - val_accuracy: 0.9667\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 133s 20s/step - loss: 0.1915 - accuracy: 0.9107 - val_loss: 0.1083 - val_accuracy: 0.9833\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 40s 6s/step - loss: 0.1559 - accuracy: 0.9464 - val_loss: 0.1141 - val_accuracy: 0.9667\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 49s 7s/step - loss: 0.1412 - accuracy: 0.9554 - val_loss: 0.0825 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 43s 6s/step - loss: 0.1535 - accuracy: 0.9375 - val_loss: 0.0574 - val_accuracy: 0.9833\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 43s 6s/step - loss: 0.1192 - accuracy: 0.9732 - val_loss: 0.0700 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 45s 7s/step - loss: 0.0994 - accuracy: 0.9643 - val_loss: 0.0713 - val_accuracy: 0.9667\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 146s 23s/step - loss: 0.9005 - accuracy: 0.4598 - val_loss: 0.6867 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 79s 12s/step - loss: 0.6521 - accuracy: 0.6339 - val_loss: 0.6100 - val_accuracy: 0.6833\n",
      "Epoch 3/10\n",
      "1/7 [===>..........................] - ETA: 35s - loss: 0.4992 - accuracy: 0.8438"
     ]
    }
   ],
   "source": [
    "#initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build()\n",
    "        \n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build()\n",
    "        local_model.compile(loss=keras.losses.binary_crossentropy,optimizer='adam',metrics=['accuracy'])\n",
    "        callback = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)]\n",
    "        local_model.fit(train_generator,epochs=10,validation_data=validation_generator,callbacks=[callback])   \n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(X_test,Y_test,global_model,100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
